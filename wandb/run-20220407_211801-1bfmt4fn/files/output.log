
Sanity Checking: 0it [00:00, ?it/s]
Global seed set to 69
Using 16bit native Automatic Mixed Precision (AMP)
/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:58: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback
  "Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be"
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name       | Type             | Params
------------------------------------------------
0 | model      | Unet             | 8.6 M
1 | loss_fn    | FocalLoss        | 0
2 | val_metric | CrossEntropyLoss | 0
------------------------------------------------
8.6 M     Trainable params
0         Non-trainable params
8.6 M     Total params
17.247    Total estimated model params size (MB)
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
Sanity Checking DataLoader 0:   0%|                                                                                                                             | 0/2 [00:23<?, ?it/s]torch.Size([64, 64, 64, 64])
torch.Size([64, 64, 64, 64])
torch.Size([64, 64, 64, 64])
torch.Size([64, 128, 32, 32])
torch.Size([64, 128, 32, 32])
torch.Size([64, 128, 32, 32])
torch.Size([64, 64, 32, 32])
in_channels: [64, 32, 32, 64, 32, 32, 32]
out_channels: [64, 64, 64, 64, 64, 64, 32, 32]
skip_channels: [64, 160, 160, 128, 96, 96, 96]
DECODER blocks: ModuleList(
  (0): DecoderBlock(
    (conv1): Conv2dReLU(
      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention1): Attention(
      (attention): Identity()
    )
    (conv2): Conv2dReLU(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention2): Attention(
      (attention): Identity()
    )
  )
  (1): DecoderBlock(
    (conv1): Conv2dReLU(
      (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention1): Attention(
      (attention): Identity()
    )
    (conv2): Conv2dReLU(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention2): Attention(
      (attention): Identity()
    )
  )
  (2): DecoderBlock(
    (conv1): Conv2dReLU(
      (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention1): Attention(
      (attention): Identity()
    )
    (conv2): Conv2dReLU(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention2): Attention(
      (attention): Identity()
    )
  )
  (3): DecoderBlock(
    (conv1): Conv2dReLU(
      (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention1): Attention(
      (attention): Identity()
    )
    (conv2): Conv2dReLU(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention2): Attention(
      (attention): Identity()
    )
  )
  (4): DecoderBlock(
    (conv1): Conv2dReLU(
      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention1): Attention(
      (attention): Identity()
    )
    (conv2): Conv2dReLU(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention2): Attention(
      (attention): Identity()
    )
  )
  (5): DecoderBlock(
    (conv1): Conv2dReLU(
      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention1): Attention(
      (attention): Identity()
    )
    (conv2): Conv2dReLU(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention2): Attention(
      (attention): Identity()
    )
  )
  (6): DecoderBlock(
    (conv1): Conv2dReLU(
      (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention1): Attention(
      (attention): Identity()
    )
    (conv2): Conv2dReLU(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (attention2): Attention(
      (attention): Identity()
    )
  )
)
features: [torch.Size([64, 64, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64])]
head before going in centerblock: torch.Size([64, 64, 32, 32])
Head after: torch.Size([64, 64, 32, 32])
-------------------------------------------------- ITERATION 0 --------------------------------------------------
skips: [torch.Size([64, 64, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64])]
Conv Layers in_ch and skip_ch: 64 & 64
x: torch.Size([64, 64, 32, 32])	skip: torch.Size([64, 64, 32, 32])
AFTER CAT: X: torch.Size([64, 128, 32, 32])	 SKIP: torch.Size([64, 64, 32, 32])
This is Conv1: Conv2dReLU(
  (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
This is Conv2: Conv2dReLU(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
-------------------------------------------------- ITERATION 1 --------------------------------------------------
skips: [torch.Size([64, 64, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64])]
Conv Layers in_ch and skip_ch: 32 & 160
x: torch.Size([64, 64, 32, 32])	skip: torch.Size([64, 128, 32, 32])
AFTER CAT: X: torch.Size([64, 192, 32, 32])	 SKIP: torch.Size([64, 128, 32, 32])
This is Conv1: Conv2dReLU(
  (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
This is Conv2: Conv2dReLU(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
-------------------------------------------------- ITERATION 2 --------------------------------------------------
skips: [torch.Size([64, 64, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64])]
Conv Layers in_ch and skip_ch: 32 & 160
x: torch.Size([64, 64, 32, 32])	skip: torch.Size([64, 128, 32, 32])
AFTER CAT: X: torch.Size([64, 192, 32, 32])	 SKIP: torch.Size([64, 128, 32, 32])
This is Conv1: Conv2dReLU(
  (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
This is Conv2: Conv2dReLU(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
-------------------------------------------------- ITERATION 3 --------------------------------------------------
skips: [torch.Size([64, 64, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64])]
Conv Layers in_ch and skip_ch: 64 & 128
x: torch.Size([64, 64, 32, 32])	skip: torch.Size([64, 128, 32, 32])
AFTER CAT: X: torch.Size([64, 192, 32, 32])	 SKIP: torch.Size([64, 128, 32, 32])
This is Conv1: Conv2dReLU(
  (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
This is Conv2: Conv2dReLU(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
-------------------------------------------------- ITERATION 4 --------------------------------------------------
skips: [torch.Size([64, 64, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64])]
Conv Layers in_ch and skip_ch: 32 & 96
++++++++Interpolated+++++++
x: torch.Size([64, 64, 64, 64])	skip: torch.Size([64, 64, 64, 64])
AFTER CAT: X: torch.Size([64, 128, 64, 64])	 SKIP: torch.Size([64, 64, 64, 64])
This is Conv1: Conv2dReLU(
  (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
This is Conv2: Conv2dReLU(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
-------------------------------------------------- ITERATION 5 --------------------------------------------------
skips: [torch.Size([64, 64, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64])]
Conv Layers in_ch and skip_ch: 32 & 96
x: torch.Size([64, 64, 64, 64])	skip: torch.Size([64, 64, 64, 64])
AFTER CAT: X: torch.Size([64, 128, 64, 64])	 SKIP: torch.Size([64, 64, 64, 64])
This is Conv1: Conv2dReLU(
  (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
This is Conv2: Conv2dReLU(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
-------------------------------------------------- ITERATION 6 --------------------------------------------------
skips: [torch.Size([64, 64, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 128, 32, 32]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64]), torch.Size([64, 64, 64, 64])]
Conv Layers in_ch and skip_ch: 32 & 96
x: torch.Size([64, 64, 64, 64])	skip: torch.Size([64, 64, 64, 64])
AFTER CAT: X: torch.Size([64, 128, 64, 64])	 SKIP: torch.Size([64, 64, 64, 64])
This is Conv1: Conv2dReLU(
  (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
This is Conv2: Conv2dReLU(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
Traceback (most recent call last):
  File "old_predator.py", line 343, in <module>
    val_dataloaders=validation_loader,
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 772, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 724, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 812, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 1237, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 1324, in _run_stage
    return self._run_train()
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 1346, in _run_train
    self._run_sanity_check()
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 1414, in _run_sanity_check
    val_loop.run()
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 153, in advance
    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 127, in advance
    output = self._evaluation_step(**kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 222, in _evaluation_step
    output = self.trainer._call_strategy_hook("validation_step", *kwargs.values())
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 1766, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/strategies/strategy.py", line 344, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "old_predator.py", line 305, in validation_step
    logits = self.forward(features)
  File "old_predator.py", line 258, in forward
    mask = self.model(image)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/segmentation_models_pytorch/base/model.py", line 18, in forward
    masks = self.segmentation_head(decoder_output)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py", line 443, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py", line 440, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [256, 64, 3, 3], expected input[64, 32, 64, 64] to have 64 channels, but got 32 channels instead
Traceback (most recent call last):
  File "old_predator.py", line 343, in <module>
    val_dataloaders=validation_loader,
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 772, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 724, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 812, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 1237, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 1324, in _run_stage
    return self._run_train()
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 1346, in _run_train
    self._run_sanity_check()
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 1414, in _run_sanity_check
    val_loop.run()
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 153, in advance
    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 127, in advance
    output = self._evaluation_step(**kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 222, in _evaluation_step
    output = self.trainer._call_strategy_hook("validation_step", *kwargs.values())
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py", line 1766, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/strategies/strategy.py", line 344, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "old_predator.py", line 305, in validation_step
    logits = self.forward(features)
  File "old_predator.py", line 258, in forward
    mask = self.model(image)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/segmentation_models_pytorch/base/model.py", line 18, in forward
    masks = self.segmentation_head(decoder_output)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py", line 443, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py", line 440, in _conv_forward
    self.padding, self.dilation, self.groups)
